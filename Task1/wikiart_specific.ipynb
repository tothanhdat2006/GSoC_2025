{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path('/kaggle/input/wikiart') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specific"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because Kaggle does not allow non-ascii in files' name, I wrote a script to rename those files and replace non-ascii characters with underscore (_). The code below was executed before uploading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import unicodedata\n",
    "# import re\n",
    "# import shutil\n",
    "\n",
    "# def normalize_filename(filename):\n",
    "#     normalized = unicodedata.normalize('NFKD', filename)\n",
    "#     ascii_filename = ''\n",
    "#     for char in normalized:\n",
    "#         if ord(char) < 128 and ord(char) != 39:\n",
    "#             ascii_filename += char\n",
    "#         else:\n",
    "#             replacements = {\n",
    "#                 'ä': 'a', 'ö': 'o', 'ü': 'u', 'ß': 'ss',\n",
    "#                 'á': 'a', 'é': 'e', 'í': 'i', 'ó': 'o', 'ú': 'u',\n",
    "#                 'à': 'a', 'è': 'e', 'ì': 'i', 'ò': 'o', 'ù': 'u',\n",
    "#                 'â': 'a', 'ê': 'e', 'î': 'i', 'ô': 'o', 'û': 'u',\n",
    "#             }\n",
    "#             ascii_filename += replacements.get(char, '_') # Replace with _ \n",
    "\n",
    "#     ascii_filename = re.sub(r'[^a-zA-Z0-9_\\-\\.]', '_', ascii_filename)\n",
    "    \n",
    "#     return ascii_filename\n",
    "\n",
    "# def rename_non_ascii_files(dir):\n",
    "#     renamed_files = []\n",
    "#     skipped_files = []\n",
    "    \n",
    "#     for art_style in os.listdir(dir):\n",
    "#         style_path = os.path.join(dir, art_style)\n",
    "#         if os.path.isdir(style_path):\n",
    "#             print(f\"Processing {art_style}...\")\n",
    "#             for file in os.listdir(style_path):\n",
    "#                 filepath = os.path.join(style_path, file)\n",
    "#                 has_non_ascii = any(ord(char) > 127 for char in file)\n",
    "#                 if has_non_ascii:\n",
    "#                     new_filename = normalize_filename(file)\n",
    "#                     new_filepath = os.path.join(style_path, new_filename)\n",
    "#                     try:\n",
    "#                         print(f\"  Renaming: {filepath} -> {new_filepath}\")\n",
    "#                         shutil.move(filepath, new_filepath)\n",
    "#                         renamed_files.append((filepath, new_filepath))\n",
    "#                     except Exception as e:\n",
    "#                         print(f\"  Error renaming {filepath}: {e}\")\n",
    "#                         skipped_files.append((filepath, str(e)))\n",
    "    \n",
    "#     print(f\"\\nRenamed {len(renamed_files)} files\")\n",
    "#     print(f\"Skipped {len(skipped_files)} files due to errors\")\n",
    "    \n",
    "#     return {\n",
    "#         \"renamed_files\": renamed_files,\n",
    "#         \"skipped_files\": skipped_files\n",
    "#     }\n",
    "\n",
    "# rename_non_ascii_files('wikiart')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, WeightedRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "TASKS_LIST = ['style', 'artist', 'genre']\n",
    "\n",
    "corrupted_images = [\n",
    "    'Baroque/rembrandt_woman-standing-with-raised-hands.jpg',\n",
    "    'Post_Impressionism/vincent-van-gogh_l-arlesienne-portrait-of-madame-ginoux-1890.jpg'\n",
    "]\n",
    "\n",
    "class TRAIN_SpecificArtGANDataset(Dataset):\n",
    "    def __init__(self, data_path, task='genre'):\n",
    "        super().__init__()\n",
    "        assert (task in TASKS_LIST), f'Task should be either {TASKS_LIST}\\n'\n",
    "        \n",
    "        self.data_path = data_path\n",
    "        self.task = task\n",
    "        self.classes_csv = pd.read_csv(data_path / f'{task}_class.txt', sep=\" \", names=['label', 'name'])\n",
    "\n",
    "        # --------------------------- Cleaning data\n",
    "        data_csv = pd.read_csv(data_path / f'{task}_train.csv', names=['filename', 'label'])\n",
    "        data_csv = data_csv.query(\"filename not in @corrupted_images\")\n",
    "        \n",
    "        import re\n",
    "        import unicodedata\n",
    "        def process_filename(f): # Remove non-ascii characters and '/' from filenames\n",
    "            dirname, filename = f.split('/', 1)\n",
    "            normalized = unicodedata.normalize('NFKD', filename)\n",
    "            ascii_filename = ''\n",
    "            for char in normalized:\n",
    "                if ord(char) < 128 and ord(char) != 39:\n",
    "                    ascii_filename += char\n",
    "                else:\n",
    "                    replacements = {\n",
    "                        'ä': 'a', 'ö': 'o', 'ü': 'u', 'ß': 'ss',\n",
    "                        'á': 'a', 'é': 'e', 'í': 'i', 'ó': 'o', 'ú': 'u',\n",
    "                        'à': 'a', 'è': 'e', 'ì': 'i', 'ò': 'o', 'ù': 'u',\n",
    "                        'â': 'a', 'ê': 'e', 'î': 'i', 'ô': 'o', 'û': 'u',\n",
    "                    }\n",
    "                    ascii_filename += replacements.get(char, '_') # Replace with _ \n",
    "        \n",
    "            ascii_filename = re.sub(r'[^a-zA-Z0-9_\\-\\.]', '_', ascii_filename)\n",
    "            return dirname + \"/\" + ascii_filename\n",
    "        \n",
    "        data_csv.loc[:, \"filename\"] = data_csv[\"filename\"].map(process_filename) # Pandas 3.0\n",
    "        self.data_csv = data_csv\n",
    "        self.imgs_path = data_csv[\"filename\"].tolist()\n",
    "        self.labels = data_csv[\"label\"].tolist()\n",
    "\n",
    "        # --------------------------- Custom transforms for train dataset\n",
    "        self.train_transforms = transforms.Compose([\n",
    "            transforms.RandomResizedCrop((224,224)),\n",
    "            transforms.RandomHorizontalFlip(0.3),\n",
    "            transforms.RandomVerticalFlip(0.3),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_path)\n",
    "\n",
    "    def get_label_name(self, label):\n",
    "        return self.classes_csv['name'][label]\n",
    "    \n",
    "    def transform(self, img):\n",
    "        img = self.train_transforms(img)\n",
    "        return img\n",
    "\n",
    "    def get_raw(self, idx):\n",
    "        img = self.imgs_path[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return {\n",
    "            'images': img,\n",
    "            'labels': label\n",
    "        }\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.data_path / self.imgs_path[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        img = self.transform(img)\n",
    "        return {\n",
    "            'images': img,\n",
    "            'labels': label\n",
    "        }\n",
    "\n",
    "\n",
    "train_dataset = TRAIN_SpecificArtGANDataset(data_path=path, task='style')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, WeightedRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "TASKS_LIST = ['style', 'artist', 'genre']\n",
    "\n",
    "corrupted_images = [\n",
    "    'Baroque/rembrandt_woman-standing-with-raised-hands.jpg',\n",
    "    'Post_Impressionism/vincent-van-gogh_l-arlesienne-portrait-of-madame-ginoux-1890.jpg'\n",
    "]\n",
    "\n",
    "class VAL_SpecificArtGANDataset(Dataset):\n",
    "    def __init__(self, data_path, task='genre'):\n",
    "        super().__init__()\n",
    "        assert (task in TASKS_LIST), f'Task should be either {TASKS_LIST}\\n'\n",
    "        \n",
    "        self.data_path = data_path\n",
    "        self.task = task\n",
    "        self.classes_csv = pd.read_csv(data_path / f'{task}_class.txt', sep=\" \", names=['label', 'name'])\n",
    "\n",
    "        # --------------------------- Cleaning data\n",
    "        data_csv = pd.read_csv(data_path / f'{task}_val.csv', names=['filename', 'label'])\n",
    "        data_csv = data_csv.query(\"filename not in @corrupted_images\")\n",
    "        \n",
    "        import re\n",
    "        import unicodedata\n",
    "        def process_filename(f):\n",
    "            dirname, filename = f.split('/', 1)\n",
    "            normalized = unicodedata.normalize('NFKD', filename)\n",
    "            ascii_filename = ''\n",
    "            for char in normalized:\n",
    "                if ord(char) < 128 and ord(char) != 39:\n",
    "                    ascii_filename += char\n",
    "                else:\n",
    "                    replacements = {\n",
    "                        'ä': 'a', 'ö': 'o', 'ü': 'u', 'ß': 'ss',\n",
    "                        'á': 'a', 'é': 'e', 'í': 'i', 'ó': 'o', 'ú': 'u',\n",
    "                        'à': 'a', 'è': 'e', 'ì': 'i', 'ò': 'o', 'ù': 'u',\n",
    "                        'â': 'a', 'ê': 'e', 'î': 'i', 'ô': 'o', 'û': 'u',\n",
    "                    }\n",
    "                    ascii_filename += replacements.get(char, '_') # Replace with _ \n",
    "        \n",
    "            ascii_filename = re.sub(r'[^a-zA-Z0-9_\\-\\.]', '_', ascii_filename)\n",
    "            return dirname + \"/\" + ascii_filename\n",
    "        \n",
    "        data_csv.loc[:, \"filename\"] = data_csv[\"filename\"].map(process_filename) # Pandas 3.0\n",
    "        self.data_csv = data_csv\n",
    "        self.imgs_path = data_csv[\"filename\"].tolist()\n",
    "        self.labels = data_csv[\"label\"].tolist()\n",
    "\n",
    "        # --------------------------- Custom transforms for valid dataset\n",
    "        self.val_transforms = transforms.Compose([\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_path)\n",
    "\n",
    "    def get_label_name(self, label):\n",
    "        return self.classes_csv['name'][label]\n",
    "    \n",
    "    def transform(self, img):\n",
    "        img = self.val_transforms(img)\n",
    "        return img\n",
    "\n",
    "    def get_label(self, idx):\n",
    "        return self.labels[idx]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.data_path / self.imgs_path[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        img = self.transform(img)\n",
    "        return {\n",
    "            'images': img,\n",
    "            'labels': label\n",
    "        }\n",
    "\n",
    "test_dataset = VAL_SpecificArtGANDataset(data_path=path, task='style')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Because of imbalanced dataset, we need a custom sampler to oversample the minority classes\n",
    "# This case, I used WeightedRandomSampler to sample class based on the class weights (which is the inverse of class frequency)\n",
    "def oversampling_dataset(labels):\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    class_weights = [1.0/c for c in counts]\n",
    "    weights_y = [class_weights[i] for i in labels]\n",
    "    return WeightedRandomSampler(weights_y, len(weights_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample_data = train_dataset[0]\n",
    "image, label = sample_data['images'], sample_data['labels']\n",
    "plt.imshow(np.array(image).transpose(1, 2, 0))\n",
    "plt.title(train_dataset.get_label_name(label))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "import torchvision.transforms.v2 as v2\n",
    "\n",
    "batch_size = 64\n",
    "num_workers = 0\n",
    "\n",
    "generator1 = torch.Generator().manual_seed(86)\n",
    "\n",
    "task = 'artist'\n",
    "train_dataset = TRAIN_SpecificArtGANDataset(data_path=path, task=task)\n",
    "val_dataset = VAL_SpecificArtGANDataset(data_path=path, task=task)\n",
    "    \n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    num_workers=num_workers, \n",
    "    sampler=oversampling_dataset(train_dataset.labels)\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=batch_size, \n",
    "    num_workers=num_workers, \n",
    "    sampler=oversampling_dataset(val_dataset.labels)\n",
    ")\n",
    "\n",
    "def labels_getter(batch):\n",
    "    return batch[1]\n",
    "    \n",
    "cutmix = v2.CutMix(num_classes=len(train_dataset.classes_csv), labels_getter=labels_getter)\n",
    "mixup = v2.MixUp(num_classes=len(train_dataset.classes_csv), labels_getter=labels_getter)\n",
    "cutmix_or_mixup = v2.RandomChoice([cutmix, mixup])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from timm import create_model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "resnet50 = create_model('resnet50', pretrained=True, num_classes=len(train_dataset.classes_csv))\n",
    "model = resnet50\n",
    "# vit_model = create_model('vit_base_patch16_224', pretrained=True, num_classes=len(train_dataset.classes_csv))\n",
    "# model = vit_model\n",
    "model.to(device)\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 3\n",
    "learning_rate = 1e-4\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.66)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    min_lr=1e-6,\n",
    "    eps=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(val_dataset), desc=f'Validating', unit='img') as pbar:\n",
    "            for batch in dataloader:\n",
    "                images, labels = batch['images'], batch['labels']\n",
    "                images = images.to(device, dtype=torch.float32, memory_format=torch.channels_last)\n",
    "                labels = labels.to(device, dtype=torch.long)\n",
    "                \n",
    "                labels_pred = model(images)\n",
    "                \n",
    "                loss = criterion(labels_pred, labels)\n",
    "                losses.append(loss.item())\n",
    "                \n",
    "                predicted = torch.argmax(torch.softmax(labels_pred, dim=1), dim=1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                pbar.update(images.shape[0])\n",
    "\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    accuracy = correct / total\n",
    "    model.train()\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "global_step = 0\n",
    "n_train = len(train_dataset)\n",
    "\n",
    "# Per epoch\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracy = []\n",
    "\n",
    "# Per step\n",
    "train_losses_steps = []\n",
    "\n",
    "best_model = model\n",
    "best_val_acc = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "Image.MAX_IMAGE_PIXELS = 1000000000   \n",
    "for epoch in range(1, n_epochs+1):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    with tqdm(total=n_train, desc=f'Epoch {epoch}/{n_epochs}', unit='img') as pbar:\n",
    "        for batch in train_dataloader:\n",
    "            images, labels = batch['images'], batch['labels']\n",
    "            # images, labels = cutmix_or_mixup(images, labels)\n",
    "            # labels = labels.argmax(dim=1)\n",
    "            images = images.to(device, dtype=torch.float32, memory_format=torch.channels_last)\n",
    "            labels = labels.to(device, dtype=torch.long)\n",
    "\n",
    "            labels_pred = model(images)\n",
    "            loss = criterion(labels_pred, labels)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            train_losses_steps.append(loss.item() / batch_size)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            global_step += 1\n",
    "            pbar.update(images.shape[0])\n",
    "            pbar.set_postfix(**{f'loss (batch)': loss.item()})\n",
    "\n",
    "\n",
    "    train_losses.append(epoch_loss / len(train_dataloader))\n",
    "    val_loss, val_acc = evaluate(model, val_dataloader, criterion, device)\n",
    "    scheduler.step(val_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracy.append(val_acc)\n",
    "    print(f'Validation loss: {val_loss}, Validation accuracy: {val_acc}')\n",
    "    if val_acc > best_val_acc:\n",
    "        best_model = model\n",
    "        best_val_acc = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting model training/validation loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(range(1, len(train_losses_steps)+1), train_losses_steps, label='Training Loss (per step)')\n",
    "plt.title('Train loss (per step)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Steps')\n",
    "plt.xticks(np.arange(1, len(train_losses_steps)+1, 100))\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(range(1, n_epochs + 1), train_losses, label='Training Loss', marker='o')\n",
    "plt.plot(range(1, n_epochs + 1), val_losses, label='Validation Loss', marker='x')\n",
    "plt.title('Train/Valid loss (per epoch)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(range(1, n_epochs + 1), val_accuracy, label='Validation Accuracy')\n",
    "plt.title('Validation accuracy (per epoch)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# state_dict = {\n",
    "#     'lr': learning_rate,\n",
    "#     'global_step': global_step,\n",
    "#     'current_epochs': epoch,\n",
    "#     'n_epochs': n_epochs,\n",
    "#     'model_state_dict': model.state_dict(),\n",
    "#     'optim_state_dict': optimizer.state_dict(),\n",
    "#     'scheduler_state_dict': scheduler.state_dict()\n",
    "# }\n",
    "\n",
    "# torch.save(state_dict, str(f'/kaggle/working/wikiart_{task}_ResNet_epoch{}.pt'.format(epoch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model_path = \"Enter model path here\"\n",
    "# state_dict = torch.load(model_path, map_location=device)\n",
    "# model.load_state_dict(state_dict['model_state_dict'])\n",
    "# optimizer.load_state_dict(state_dict['optim_state_dict'])\n",
    "# scheduler.load_state_dict(state_dict['scheduler_state_dict'])\n",
    "# global_step = state_dict['global_step']\n",
    "# epoch = state_dict['current_epochs']\n",
    "# n_epochs = state_dict['n_epochs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Use validation dataset for testing\n",
    "test_dataset = VAL_SpecificArtGANDataset(data_path=path, task='artist')\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size, \n",
    "    num_workers=num_workers,\n",
    "    sampler=oversampling_dataset(test_dataset.labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score, accuracy_score\n",
    "\n",
    "all_pred, all_label, all_label_pred = [], [], []\n",
    "def evaluate_test(model, dataloader, device):\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    global all_pred, all_label, all_label_pred\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(test_dataset), desc=f'Testing', unit='img') as pbar:\n",
    "            for batch in dataloader:\n",
    "                images, labels = batch['images'], batch['labels']\n",
    "                images = images.to(device, dtype=torch.float32, memory_format=torch.channels_last)\n",
    "                labels = labels.to(device, dtype=torch.long)\n",
    "                \n",
    "                labels_pred = model(images)\n",
    "                labels_pred = nn.Softmax(dim=1)(labels_pred)\n",
    "                all_label_pred.append(labels_pred.cpu().numpy())\n",
    "\n",
    "                predicted = torch.argmax(labels_pred, dim=1) # Get the index of the class with the highest probability\n",
    "                all_pred.append(predicted.cpu().numpy()) \n",
    "                all_label.append(labels.cpu().numpy())\n",
    "                pbar.update(images.shape[0])\n",
    "\n",
    "    all_pred = np.concatenate(all_pred)\n",
    "    all_label = np.concatenate(all_label)\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(all_label, all_pred, labels=np.arange(0, len(test_dataset.classes_csv)))\n",
    "    cmDisplay = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.arange(0, len(test_dataset.classes_csv)))\n",
    "    fig, ax = plt.subplots(figsize=(15,15))\n",
    "    cmDisplay.plot(ax=ax)\n",
    "\n",
    "    # Calculate metrics\n",
    "    print(f\"Accuracy: {accuracy_score(all_label, all_pred, normalize=True)}\")\n",
    "    print(f\"Micro F1: {f1_score(all_label, all_pred, average='micro')}\")\n",
    "    print(f\"Weighted F1: {f1_score(all_label, all_pred, average='weighted')}\")\n",
    "    model.train()\n",
    "\n",
    "evaluate_test(model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "true_labels = [test_dataset.get_label_name(label) for label in test_dataset.classes_csv['label']]\n",
    "\n",
    "print(classification_report(all_label, all_pred, target_names=test_dataset.classes_csv['name'].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradCAM (for ViT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install grad-cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "# from pytorch_grad_cam import GradCAM\n",
    "# from pytorch_grad_cam.utils.reshape_transforms import vit_reshape_transform\n",
    "\n",
    "# target_labels=[1, 2, 4, 5, 6]\n",
    "# examples_per_label=2\n",
    "# label_examples = {label: [] for label in target_labels}\n",
    "\n",
    "# for batch in test_dataloader:\n",
    "#     imgs, labels = batch['images'], batch['labels']\n",
    "#     for i, label in enumerate(labels):\n",
    "#         label_item = label.item()\n",
    "#         if label_item in target_labels and len(label_examples[label_item]) < examples_per_label:\n",
    "#             label_examples[label_item].append(imgs[i])\n",
    "\n",
    "#     if all(len(label_examples[label]) == examples_per_label for label in target_labels):\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# target_layers = [model.blocks[-1].norm1]\n",
    "# cam = GradCAM(model=model, target_layers=target_layers, reshape_transform=vit_reshape_transform)\n",
    "\n",
    "# for label, imgs in label_examples.items():\n",
    "#     for i, img in enumerate(imgs):\n",
    "#         input_tensor = img.unsqueeze(0).to(device)\n",
    "#         grayscale_cam = cam(input_tensor=input_tensor, targets=None)\n",
    "#         grayscale_cam = grayscale_cam[0, :]\n",
    "        \n",
    "#         # Convert the image tensor to a NumPy array for visualization.\n",
    "#         # Assumes the image is in (C, H, W) format and normalized between 0 and 1.\n",
    "#         img_np = img.cpu().numpy().transpose(1, 2, 0)\n",
    "        \n",
    "#         heatmap = cv2.applyColorMap(np.uint8(255 * grayscale_cam), cv2.COLORMAP_JET)\n",
    "#         heatmap = np.float32(heatmap) / 255\n",
    "        \n",
    "#         # Overlay the heatmap on the original image.\n",
    "#         overlay = heatmap + np.float32(img_np)\n",
    "#         overlay = overlay / np.max(overlay)\n",
    "        \n",
    "#         plt.figure(figsize=(12, 4))\n",
    "#         plt.subplot(1, 3, 1)\n",
    "#         plt.imshow(img_np)\n",
    "#         plt.title(\"Original Image\")\n",
    "#         plt.axis(\"off\")\n",
    "        \n",
    "#         plt.subplot(1, 3, 2)\n",
    "#         plt.imshow(grayscale_cam, cmap=\"jet\")\n",
    "#         plt.title(\"GradCAM Heatmap\")\n",
    "#         plt.axis(\"off\")\n",
    "        \n",
    "#         plt.subplot(1, 3, 3)\n",
    "#         plt.imshow(overlay)\n",
    "#         plt.title(\"Overlay\")\n",
    "#         plt.axis(\"off\")\n",
    "        \n",
    "#         plt.suptitle(f\"Label: {label} | Example: {i+1}\", fontsize=14)\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6911215,
     "isSourceIdPinned": true,
     "sourceId": 11088051,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 280556,
     "modelInstanceId": 259369,
     "sourceId": 303864,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
